{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582a52c-cc3e-4a42-8673-c49db5efc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7904fb-d94f-4ce9-a4bd-ef0b40728db3",
   "metadata": {},
   "source": [
    "# Create Smaller Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141b3c7-c417-4ed1-bb01-e1ec4342d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cc100/pl_5e7.txt', 'w') as f:\n",
    "    for i, line in enumerate(open('data/cc100/pl.txt', 'r')):\n",
    "        if i >= 5e7:\n",
    "            break\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c228ab0-b235-41b2-a204-85dafbd5cb5c",
   "metadata": {},
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37701237-192b-41ad-b658-abc8055d790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0350b0-e1e1-4baa-aefa-3ca9e7d9c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IN = 'data/cc100/pl_5e7.txt'\n",
    "PATH_OUT = 'data/cc100_filtered_5e7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930da26-441a-46fc-912c-d5ffe15284d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(path_in, path_out):\n",
    "    raw_datasets = load_dataset('text', data_files=path_in)\n",
    "    NUM_PROC = multiprocessing.cpu_count()\n",
    "    \n",
    "    import re\n",
    "    import html as ihtml\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = BeautifulSoup(ihtml.unescape(text), \"lxml\").text\n",
    "        text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    filter_non_alfanum = lambda x: re.sub('[^0-9AaĄąBbCcĆćDdEeĘęFfGgHhIiJjKkLlŁłMmNnŃńOoÓóPpRrSsŚśTtUuWwYyZzŹźŻż\\,\\. ]+', '', x)\n",
    "    filter_ratio = lambda x: len(filter_non_alfanum(x)) / len(x)\n",
    "    \n",
    "    raw_datasets = raw_datasets.filter(lambda x: len(x['text']) > 15, num_proc=NUM_PROC)\n",
    "    raw_datasets = raw_datasets.map(lambda x: {'text':  [clean_text(y) for y in x['text']]}, batched=True, num_proc=NUM_PROC)\n",
    "    raw_datasets = raw_datasets.filter(lambda x: len(x['text']) > 15 and filter_ratio(x['text']) > 0.9, num_proc=NUM_PROC)\n",
    "    raw_datasets.save_to_disk(path_out)\n",
    "    \n",
    "preprocess_dataset(PATH_IN, PATH_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63263ee5-2117-4bb3-8acd-aa6bc11f9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dedup_datasets = load_from_disk(PATH_OUT)\n",
    "dedup_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f571b7-12f8-4e2d-88f5-237f8bcbe9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_datasets.shuffle()['train'].select(range(5))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1874ed7-c0f1-41b9-9c72-a7f665f732fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.herbert.tokenization_herbert_fast import HerbertTokenizerFast\n",
    "tokenizer = HerbertTokenizerFast.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6f73c-6197-41ef-bd5c-24d548f9ae15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#after_deduplication\n",
    "import glob\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "dedup_datasets = [load_dataset('json', data_files=path)['train'] for path in glob.glob('./datasets/data/*.json.gz')]\n",
    "dedup_dataset = concatenate_datasets(dedup_datasets)\n",
    "dedup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70da11-b75f-40ed-b09f-8513452d3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver1\n",
    "def tokenize_dataset1(dedup_dataset, path_tokenized_out):\n",
    "    NUM_PROC = multiprocessing.cpu_count()\n",
    "    def tokenize_function(example):\n",
    "        tokenized = tokenizer(example['text'], truncation=True)\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_dataset = dedup_dataset.map(tokenize_function, batched=True, num_proc=NUM_PROC)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['text', 'token_type_ids'])\n",
    "    tokenized_dataset = tokenized_dataset.with_format('torch')\n",
    "    tokenized_dataset = tokenized_dataset['train'].train_test_split(test_size=0.01, seed=29)\n",
    "    print(tokenized_dataset)\n",
    "    tokenized_dataset.save_to_disk(path_tokenized_out)\n",
    "    \n",
    "tokenize_dataset1(dedup_datasets, 'data/tokenized_dataset_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a13585-6c27-4d68-8d51-b91409ab89dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ver2\n",
    "def get_proper_idx1(idx, context_length, words_ids):\n",
    "    if idx + context_length >= len(words_ids) - 1:\n",
    "        return idx + context_length, idx + context_length\n",
    "    if words_ids[idx + context_length] != words_ids[idx + context_length - 1]:\n",
    "        return idx + context_length, idx + context_length\n",
    "    else:\n",
    "        while words_ids[idx + context_length] == words_ids[idx + context_length - 1]:\n",
    "            idx -= 1\n",
    "        return idx + context_length, idx + context_length\n",
    "            \n",
    "def get_proper_idx2(idx, context_length, words_ids):\n",
    "    if idx + context_length >= len(words_ids) - 1:\n",
    "        return idx + context_length, idx + context_length\n",
    "    if words_ids[idx + context_length - 1] == None:\n",
    "        return idx + context_length, idx + context_length\n",
    "    else:\n",
    "        while words_ids[idx + context_length] == words_ids[idx + context_length - 1]:\n",
    "            idx -= 1\n",
    "        lidx = idx\n",
    "        ridx = idx\n",
    "        while words_ids[lidx + context_length - 1] != None:\n",
    "            lidx -= 1\n",
    "        while words_ids[ridx + context_length - 1] != None:\n",
    "            ridx += 1\n",
    "        lidx = lidx + context_length\n",
    "        ridx = ridx + context_length\n",
    "        idx = idx + context_length\n",
    "        \n",
    "        if idx - lidx < 20:\n",
    "            return lidx, lidx\n",
    "        elif ridx - idx < 20:\n",
    "            return idx, ridx\n",
    "        else:\n",
    "            return idx, idx\n",
    "            \n",
    "\n",
    "def tokenize_dataset2(dedup_dataset, path_tokenized_out, context_length=400):\n",
    "    NUM_PROC = multiprocessing.cpu_count()\n",
    "    # nie dodawaj tokenów specjalnych\n",
    "    def tokenize_function(example):\n",
    "        all_input_ids = [0]\n",
    "        all_words_ids = [None]\n",
    "        results = tokenizer(example['text'], add_special_tokens=False)\n",
    "        for i, input_ids in enumerate(results['input_ids']):\n",
    "            all_input_ids.extend(input_ids)\n",
    "            all_input_ids.append(tokenizer.sep_token_id)\n",
    "            \n",
    "            all_words_ids.extend(results.word_ids(i))\n",
    "            all_words_ids.append(None)\n",
    "        chunks1 = []\n",
    "        i = 0\n",
    "        while i < len(all_input_ids):\n",
    "            j_min, j_max = get_proper_idx2(i, context_length, all_words_ids)\n",
    "            # problem z ucinaniem słow\n",
    "            chunks1.append([0] + all_input_ids[i: j_min])\n",
    "            i = j_max\n",
    "        return {'input_ids': chunks1}\n",
    "\n",
    "    tokenized_dataset = dedup_dataset.map(tokenize_function, batched=True, num_proc=NUM_PROC, remove_columns=['text'])\n",
    "    # tokenized_dataset = tokenized_dataset.remove_columns(['text', 'token_type_ids'])\n",
    "    # tokenized_dataset = tokenized_dataset.with_format('torch')\n",
    "    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x['input_ids']) >= 30, num_proc=NUM_PROC)\n",
    "    tokenized_dataset = tokenized_dataset['train'].train_test_split(test_size=0.01, seed=29)\n",
    "    print(tokenized_dataset)\n",
    "    tokenized_dataset.save_to_disk(path_tokenized_out)\n",
    "    \n",
    "tokenize_dataset2(dedup_datasets, 'data/tokenized_dataset_5e7', context_length=tokenizer.model_max_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd779efd-1694-46a1-951a-cc75297447a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from models.collator import DataCollatorForWholeWordMask\n",
    "BATCH_SIZE = 8\n",
    "def get_dataloaders(tokenizer, path_tokenized_dataset):\n",
    "    tokenized_datasets = load_from_disk(path_tokenized_dataset)\n",
    "    train_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer)\n",
    "    test_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer)\n",
    "    train_set = tokenized_datasets['train']\n",
    "    test_set = tokenized_datasets['test']\n",
    "    train = DataLoader(dataset=train_set, shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collator)\n",
    "    test = DataLoader(dataset=test_set, shuffle=False, batch_size=BATCH_SIZE, collate_fn=test_collator)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(tokenizer, 'data/tokenized_dataset_demo2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89da5ce-49c5-4b69-8e05-1dfafd8bf228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c3d36-8542-4a00-b6dd-f423d45cdfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk('data/tokenized_dataset_demo2')\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a9b99-eb2b-44ac-9c9d-f23d6011f395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][4]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27395b1-baac-4555-8daa-1e392f8b37c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5065d-089e-4b10-a209-d4ca1d9e5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3a1a4-d778-48ca-99ca-1268ce10691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I&#039;m a transformer called BERT\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675ae1c-c85d-42c6-9557-fcdcf5445116",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<div>\n",
    "<h1>Title</h1>\n",
    "<p>A long text........ </p>\n",
    "<a href=\"\"> a link </a>\n",
    "</div>\"\"\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72de8c1-d5b1-4e06-940c-f679bacae0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.unescape(html.escape(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85c9fc-c2a2-4048-9cc2-b2a80766d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46efa5-9c21-4108-8e33-1e4ae42fc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(html.unescape(text), \"lxml\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18612546-b6fd-43f8-9190-6e78b220abb5",
   "metadata": {},
   "source": [
    "# Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a1cee-01aa-44dc-8924-704a103c4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk('data/tokenized_dataset_5e7')\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e71035-f4d0-4eb8-99a6-501f379ade50",
   "metadata": {},
   "source": [
    "### Num of Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce655816-4d7e-4363-acd8-b6939e030148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a76f44-963c-446b-a410-7ad56f014cbd",
   "metadata": {},
   "source": [
    "### Num of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1545cac-0ead-480b-8190-99f688e64135",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tokens = 0\n",
    "for input_ids in tokenized_datasets['train']['input_ids']:\n",
    "    nb_tokens += len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189d760-0901-46f4-a2de-03e3acf5b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tokens = sum([len(input_ids) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166036d2-7190-49f7-ad52-9ebf71e72ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f185916-12be-4971-878e-1e78bcd8d180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
